\documentclass[a4paper, 12pt]{report}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage[english]{babel}
\usepackage[noheader]{packages/sleek}
\usepackage{packages/sleek-title}
\usepackage{packages/sleek-theorems}
\usepackage{packages/sleek-listings}
\graphicspath{{./resources/img/}}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

%\logo{siliconblast}
%\institute{Random University}
%\faculty{SiliconBlast Research}
%\department{Department of Anything but Psychology}
\title{ArteriaAI: Document Understanding Product}
%\subtitle{Technical Notes}
\author{\textit{Author}\\Ekaba \textsc{Bisong}}
%\supervisor{Linus \textsc{Torvalds}}
%\context{Well, I was bored...}
\date{}

%%%%%%%%%%%%%%%%
% Bibliography %
%%%%%%%%%%%%%%%%

\addbibresource{./resources/bib/references.bib}

%%%%%%%%%%
% Others %
%%%%%%%%%%

\lstdefinestyle{latex}{
    language=TeX,
    style=default,
    %%%%%
    commentstyle=\ForestGreen,
    keywordstyle=\TrueBlue,
    stringstyle=\VeronicaPurple,
    emphstyle=\TrueBlue,
    %%%%%
    emph={LaTeX, usepackage, textit, textbf, textsc}
}

\FrameTBStyle{latex}

\def\tbs{\textbackslash}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
\maketitle
\romantableofcontents


%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

\section{Background and Motivation}
In the rapidly evolving landscape of data-driven applications, the need for efficient, scalable, and cost-effective document processing and retrieval systems has become increasingly critical. Organizations across various sectors are grappling with the challenge of extracting meaningful insights from vast repositories of unstructured data, particularly in the form of PDF documents. This paper presents a novel system design that addresses these challenges, leveraging cutting-edge cloud technologies and machine learning techniques to create a robust, serverless architecture for document processing and intelligent search.

\section{Problem Statement}
The primary challenge this system aims to address is the efficient extraction, storage, and retrieval of information from large volumes of PDF documents. Traditional approaches often struggle with:
\begin{itemize}
    \item Scalability issues when processing large numbers of documents
    \item High latency in search and retrieval operations
    \item Inefficient use of computational resources, leading to increased operational costs
    \item Limited semantic understanding of document contents, resulting in suboptimal search results
\end{itemize}
Our system design seeks to overcome these limitations by employing a serverless architecture, advanced natural language processing techniques, and optimized storage solutions.

\section{Objectives}
The key objectives of this system design are:
\begin{enumerate}
    \item To develop a scalable and cost-effective solution for processing and storing large volumes of PDF documents
    \item To implement an intelligent search functionality that understands the semantic content of documents
    \item To minimize operational costs through efficient resource utilization and serverless architecture
    \item To ensure high availability and low latency in document retrieval operations
    \item To maintain robust security measures for sensitive data and API keys
\end{enumerate}

\section{Scope and Limitations}
This paper focuses on the design of a cloud-based system for PDF document processing and retrieval. The scope includes:
\begin{itemize}
    \item PDF text extraction and processing
    \item Generation and storage of text embeddings
    \item Implementation of vector-based similarity search
    \item Integration of serverless computing for scalable processing
    \item Secure management of sensitive information and API keys
\end{itemize}
While the system is designed to handle PDF documents, the principles and architecture discussed could be extended to other document formats with appropriate modifications.

\section{Methodology}
Our approach combines several state-of-the-art technologies and methodologies:
\begin{itemize}
    \item Serverless computing using Google Cloud Run for scalable document processing
    \item Vector embeddings generated using advanced natural language processing models
    \item Hybrid storage strategy utilizing Google Cloud Firestore and Pinecone vector database
    \item RESTful API design principles for system interaction
    \item Secure secret management using Google Cloud Secret Manager
\end{itemize}
These components are integrated into a cohesive system that balances performance, cost-effectiveness, and scalability.

Through this comprehensive exploration, we aim to contribute to the field of cloud-based document processing and retrieval systems, offering insights into building efficient, scalable, and cost-effective solutions for managing and extracting value from large document repositories.

\chapter{System Design}

\section{Storage and Retrieval Strategy for Vector Embeddings and Text Chunks}
In designing a robust system for managing vector embeddings and associated text chunks, it is imperative to strike an optimal balance between latency, cost-effectiveness, and scalability. This section presents a comprehensive analysis of various storage options, culminating in a justified recommendation for the most suitable architecture.

\subsection{Options for Storage}
\begin{enumerate}
    \item In-Memory Storage:
    \begin{itemize}
        \item Pros: Extremely fast access, low latency.
        \item Cons: Limited by memory capacity, not suitable for large-scale data, data is lost on restart.
    \end{itemize}
    \item File Storage (e.g., Cloud Storage):
    \begin{itemize}
        \item Pros: Cost-effective, easy to implement, scalable.
        \item Cons: Higher latency for read/write operations, limited querying capabilities.
    \end{itemize}
    \item Database Storage (e.g., Firestore, Bigtable, or a Vector Database like Pinecone):
    \begin{itemize}
        \item Pros: Scalable, provides efficient querying and indexing, good for structured data, supports complex queries.
        \item Cons: Higher cost, moderate latency compared to in-memory storage.
    \end{itemize}
\end{enumerate}

\section{Recommended Approach: Hybrid Storage with Firestore and Pinecone}

By using Firestore for metadata and Pinecone for embeddings, the system can quickly retrieve metadata and perform low-latency searches for embeddings. This hybrid approach leverages the cost-effectiveness of Firestore for structured data and the specialized capabilities of Pinecone for vector data. Both Firestore and Pinecone are designed to scale automatically, ensuring the system can handle increasing amounts of data without significant performance degradation. In essence, we'll use Firestore for Metadata and Text Chunks. This will store the text chunks and associated metadata and allow for efficient querying and retrieval of text and metadata. In using Pinecone (or any other vector database) for embeddings we can store the vector embeddings efficiently. And vector databases supports low-latency similarity searches and vector operations.

\subsection{Firestore for Metadata and Text Chunks}
\begin{itemize}
    \item \textbf{Real-time Capabilities:} Firestore's real-time database functionality enables swift access to metadata and text chunks, crucial for maintaining system responsiveness.
    \item \textbf{Low-latency Operations:} The database provides exceptionally low-latency read and write operations, particularly beneficial for structured data retrieval.
    \item \textbf{Cost-effective Scaling:} Firestore's pay-as-you-go pricing model ensures cost-effectiveness, especially for variable workloads, aligning well with dynamic system requirements.
    \item \textbf{Automatic Scalability:} The database is engineered to scale automatically with increasing data volume and read/write operations, eliminating the need for manual scaling interventions.
\end{itemize}

\subsection{Pinecone (or any other Vector Database) for Embeddings}
\begin{itemize}
    \item \textbf{Vector Optimization:} Pinecone's architecture is specifically optimized for vector operations, ensuring high efficiency in storing and querying embeddings.
    \item \textbf{Low-latency Similarity Searches:} The database excels in performing low-latency similarity searches, a critical feature for applications relying on rapid embedding retrieval.
    \item \textbf{Performance-Cost Balance:} While potentially incurring higher costs than general-purpose databases, Pinecone's performance benefits for vector data often justify the investment.
    \item \textbf{Seamless Scalability:} Pinecone is designed to handle large-scale embedding data, offering seamless scalability as the dataset expands.
\end{itemize}

\subsection{Why Not In-Memory Storage?}
\begin{enumerate}
    \item \textbf{Limited by Memory Capacity:} In-memory storage is not suitable for large-scale data as it is limited by the available memory.
    \item \textbf{Data Volatility:} Data stored in memory is lost if the service is restarted or crashes, which is not ideal for persistent storage requirements.
    \item \textbf{Scalability:} Managing large volumes of data in memory becomes impractical and expensive as the dataset grows.
\end{enumerate}


\section{Secure Secret Management with Google Secret Manager}
In the architecture of our system, the secure management of sensitive information is paramount. To address this critical requirement, we have integrated Google Secret Manager, a robust and scalable solution for storing and managing confidential data.

\subsection{Rationale for Adoption}
The implementation of Google Secret Manager in our system design is predicated on several key factors:

\begin{enumerate}
    \item \textbf{Enhanced Security Posture:} Google Secret Manager employs state-of-the-art encryption protocols for data at rest and in transit, leveraging Google's advanced security infrastructure. This significantly mitigates the risk of unauthorized access to sensitive information.
    \item \textbf{Granular Access Control:} The solution integrates seamlessly with Google Cloud Identity and Access Management (IAM), enabling fine-grained control over secret access. This allows for the implementation of the principle of least privilege, ensuring that entities within the system have access only to the secrets they require for their specific functions.

    \item \textbf{Version Control and Auditing:} The versioning capability of Secret Manager facilitates the management of secret lifecycles, allowing for easy rollbacks and historical tracking. Coupled with comprehensive auditing features, this provides a clear trail of secret access and modifications, enhancing our system's compliance with security best practices.

    \item \textbf{Seamless Integration:} Given our system's reliance on Google Cloud Platform services such as Firestore, Secret Manager offers native integration, streamlining the process of secret retrieval and management across our application ecosystem.

    \item \textbf{Centralized Management:} By providing a unified platform for secret storage, Secret Manager reduces the operational complexity associated with managing secrets across disparate system components. This centralization minimizes the attack surface and simplifies secret rotation and revocation procedures.
\end{enumerate}

\subsection{Implementation Strategy}
In our system, Google Secret Manager is utilized to securely store and manage a variety of sensitive data, including:

\begin{itemize}
    \item API authentication tokens for external services (e.g., Pinecone API keys)
    \item Database connection strings and credentials (e.g., Firestore access parameters)
    \item Encryption keys used for data protection within the application
    \item Environment-specific configuration data containing sensitive information
\end{itemize}

The application architecture is designed to retrieve these secrets at runtime, adhering to the principle of dynamic secret management. This approach ensures that sensitive data is never hard-coded or stored in configuration files, significantly reducing the risk of inadvertent exposure through code repositories or configuration management systems.


\section{Serverless Deployment with Google Cloud Run}

In the pursuit of an optimal balance between performance, scalability, and cost-effectiveness, our system leverages Google Cloud Run for serverless deployment. This choice is pivotal in achieving a highly responsive yet economically efficient architecture.

\subsection{Scalability and Performance}
Cloud Run, a fully managed serverless platform, offers several key advantages:

\begin{itemize}
    \item \textbf{Automatic Scaling:} Cloud Run dynamically adjusts the number of container instances based on incoming traffic, ensuring optimal resource utilization.
    \item \textbf{Rapid Response:} The platform can quickly spin up new instances to handle sudden spikes in demand, maintaining low latency even under variable load conditions.
    \item \textbf{Stateless Architecture:} By design, Cloud Run encourages stateless applications, promoting better scalability and easier management of distributed systems.
\end{itemize}

\subsection{Scale-to-Zero Capability}
A distinguishing feature of Cloud Run is its ability to scale the number of running instances to zero when the service is not in use. This capability brings several benefits:

\begin{enumerate}
    \item \textbf{Cost Optimization:} When there are no incoming requests, the service scales down to zero instances, effectively eliminating idle resource costs.
    \item \textbf{Resource Efficiency:} Computing resources are allocated only when needed, aligning perfectly with the principles of efficient resource management in cloud environments.
    \item \textbf{Environmental Consideration:} By minimizing unnecessary compute usage, the scale-to-zero approach contributes to reduced energy consumption, aligning with sustainable computing practices.
\end{enumerate}

\subsection{Economic Impact Analysis}
The implementation of Cloud Run with its scale-to-zero capability presents significant economic advantages:

\begin{itemize}
    \item \textbf{Pay-per-Use Model:} Costs are incurred only for the actual compute time used to process requests, not for idle time. This model is particularly beneficial for services with variable or unpredictable traffic patterns.
    \item \textbf{Reduction in Operational Overhead:} The serverless nature of Cloud Run eliminates the need for server provisioning, capacity planning, and maintenance, reducing operational costs and complexity.
    \item \textbf{Optimized Resource Allocation:} By automatically adjusting resources based on demand, the system avoids over-provisioning, a common cause of inflated cloud expenses.
\end{itemize}

\subsection{Integration with System Architecture}
Cloud Run seamlessly integrates with other components of our system:

\begin{itemize}
    \item \textbf{Firestore and Pinecone Interaction:} The stateless nature of Cloud Run instances complements the use of Firestore for metadata and Pinecone for vector embeddings, allowing for efficient, scalable data operations.
    \item \textbf{Secret Management:} Cloud Run's integration with Google Secret Manager ensures secure access to sensitive information, maintaining the system's security posture even in a serverless environment.
    \item \textbf{Event-Driven Processing:} In conjunction with Google Cloud Pub/Sub, Cloud Run enables efficient event-driven document processing, scaling resources precisely in response to incoming documents.
\end{itemize}

\subsection{Performance Considerations}
While the scale-to-zero feature offers significant benefits, it's important to address potential trade-offs:

\begin{itemize}
    \item \textbf{Cold Start Latency:} When scaling from zero, there can be a slight delay in spinning up new instances. This "cold start" phenomenon is mitigated by Cloud Run's rapid instance initialization, typically completed within seconds.
    \item \textbf{Warm Instance Retention:} To balance between cost savings and performance, Cloud Run allows configuration of minimum instances, ensuring a baseline of warm instances for immediate request handling.
\end{itemize}

The integration of Google Cloud Run with its scale-to-zero capability into our system architecture represents a strategic decision that harmonizes performance requirements with cost optimization. This approach not only ensures efficient resource utilization and significant cost savings but also positions the system to handle varying workloads with agility and economic prudence.

\section{Event-Driven Processing with Cloud Pub/Sub}

To ensure efficient and scalable processing of newly uploaded documents, our system leverages Google Cloud Pub/Sub in conjunction with Cloud Storage and Cloud Run. This event-driven architecture allows for automatic triggering of document processing functions, optimizing resource utilization and enhancing system responsiveness.

\subsection{Integration of Cloud Storage, Pub/Sub, and Cloud Run}

The workflow for processing new documents is as follows:

\begin{enumerate}
    \item \textbf{Document Upload:} A new PDF document is uploaded to a designated Cloud Storage bucket.
    \item \textbf{Event Generation:} Cloud Storage automatically generates a notification event upon successful upload.
    \item \textbf{Pub/Sub Publication:} This event is published to a predefined Pub/Sub topic.
    \item \textbf{Cloud Run Trigger:} A Cloud Run service subscribed to this Pub/Sub topic is automatically invoked.
    \item \textbf{Document Processing:} The Cloud Run service initiates the document processing pipeline.
\end{enumerate}

\subsection{Advantages of This Approach}

\begin{itemize}
    \item \textbf{Decoupling:} Pub/Sub decouples the document upload process from the processing logic, allowing each component to scale independently.
    \item \textbf{Asynchronous Processing:} Documents can be uploaded and processed asynchronously, preventing upload bottlenecks during high-traffic periods.
    \item \textbf{Scalability:} The system can handle a large number of simultaneous uploads by distributing the processing load across multiple Cloud Run instances.
    \item \textbf{Reliability:} Pub/Sub's at-least-once delivery guarantee ensures that no uploaded document goes unprocessed.
    \item \textbf{Cost-Efficiency:} Processing resources are utilized only when needed, aligning with Cloud Run's scale-to-zero capability.
\end{itemize}

\subsection{Implementation Details}

\subsubsection{Cloud Storage Configuration}
The Cloud Storage bucket is configured to send notifications to Pub/Sub when new objects are created:

\begin{verbatim}
gsutil notification create -t [TOPIC_NAME] -f json gs://[BUCKET_NAME]
\end{verbatim}

\subsubsection{Pub/Sub Topic and Subscription}
A Pub/Sub topic is created to receive Cloud Storage notifications, and a subscription is set up for the Cloud Run service:

\begin{verbatim}
gcloud pubsub topics create [TOPIC_NAME]
gcloud pubsub subscriptions create [SUBSCRIPTION_NAME] --topic [TOPIC_NAME]
\end{verbatim}

\subsubsection{Cloud Run Service}
The Cloud Run service is configured to receive Pub/Sub messages. The service extracts the Cloud Storage event data from the Pub/Sub message, retrieves the newly uploaded document, and initiates the processing pipeline.

\begin{verbatim}
gcloud run deploy [SERVICE_NAME] \
  --image [IMAGE_URL] \
  --platform managed \
  --region [REGION] \
  --set-env-vars PROJECT_ID=[PROJECT_ID] \
  --allow-unauthenticated
\end{verbatim}

\subsection{Error Handling and Retry Mechanism}
To ensure robustness, the system implements comprehensive error handling:

\begin{itemize}
    \item If document processing fails, the Pub/Sub message is not acknowledged, triggering automatic redelivery.
    \item A dead-letter topic is configured for messages that repeatedly fail processing, allowing for manual investigation and reprocessing.
\end{itemize}

\subsection{Monitoring and Logging}
To maintain system health and facilitate troubleshooting:

\begin{itemize}
    \item Cloud Monitoring is used to track Pub/Sub message throughput and Cloud Run invocations.
    \item Detailed logging is implemented at each stage of the process, from document upload to completion of processing.
    \item Alerts are set up for anomalies such as high message backlog or increased processing failures.
\end{itemize}

This event-driven architecture, powered by Cloud Pub/Sub, ensures that our system can efficiently handle document uploads at scale, while maintaining the cost-effectiveness and flexibility offered by serverless computing. It seamlessly integrates with our existing Cloud Run infrastructure, reinforcing the system's capability to process documents with high throughput and low latency.



\end{document}
